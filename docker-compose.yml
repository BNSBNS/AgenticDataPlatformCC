version: '3.8'

# ==============================================================================
# AGENTIC DATA PLATFORM - LOCAL DEVELOPMENT ENVIRONMENT
# ==============================================================================
# This Docker Compose file provides a complete local development environment
# including Kafka, Flink, MinIO, PostgreSQL, Qdrant, and monitoring tools.
#
# Usage:
#   docker-compose up -d         # Start all services
#   docker-compose down          # Stop all services
#   docker-compose logs -f       # View logs
#   docker-compose ps            # Check service status
# ==============================================================================

services:
  # ============================================================================
  # ZOOKEEPER - Kafka dependency
  # ============================================================================
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper-data:/var/lib/zookeeper/data
      - zookeeper-logs:/var/lib/zookeeper/log
    networks:
      - dataplatform

  # ============================================================================
  # KAFKA BROKERS - Event streaming
  # ============================================================================
  kafka-1:
    image: confluentinc/cp-kafka:7.5.0
    hostname: kafka-1
    container_name: kafka-1
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "19092:19092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-1:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost
    volumes:
      - kafka-1-data:/var/lib/kafka/data
    networks:
      - dataplatform

  kafka-2:
    image: confluentinc/cp-kafka:7.5.0
    hostname: kafka-2
    container_name: kafka-2
    depends_on:
      - zookeeper
    ports:
      - "9093:9093"
      - "19093:19093"
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-2:29093,PLAINTEXT_HOST://localhost:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_JMX_PORT: 9102
      KAFKA_JMX_HOSTNAME: localhost
    volumes:
      - kafka-2-data:/var/lib/kafka/data
    networks:
      - dataplatform

  kafka-3:
    image: confluentinc/cp-kafka:7.5.0
    hostname: kafka-3
    container_name: kafka-3
    depends_on:
      - zookeeper
    ports:
      - "9094:9094"
      - "19094:19094"
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-3:29094,PLAINTEXT_HOST://localhost:9094
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_JMX_PORT: 9103
      KAFKA_JMX_HOSTNAME: localhost
    volumes:
      - kafka-3-data:/var/lib/kafka/data
    networks:
      - dataplatform

  # ============================================================================
  # SCHEMA REGISTRY - Schema management for Kafka
  # ============================================================================
  schema-registry:
    image: confluentinc/cp-schema-registry:7.5.0
    hostname: schema-registry
    container_name: schema-registry
    depends_on:
      - kafka-1
      - kafka-2
      - kafka-3
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'kafka-1:29092,kafka-2:29093,kafka-3:29094'
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
    networks:
      - dataplatform

  # ============================================================================
  # MINIO - S3-compatible object storage
  # ============================================================================
  minio:
    image: minio/minio:latest
    hostname: minio
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - minio-data:/data
    networks:
      - dataplatform
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3

  # MinIO Client - Create buckets on startup
  minio-init:
    image: minio/mc:latest
    container_name: minio-init
    depends_on:
      - minio
    entrypoint: >
      /bin/sh -c "
      sleep 5;
      /usr/bin/mc alias set myminio http://minio:9000 minioadmin minioadmin;
      /usr/bin/mc mb myminio/lakehouse-bronze --ignore-existing;
      /usr/bin/mc mb myminio/lakehouse-silver --ignore-existing;
      /usr/bin/mc mb myminio/lakehouse-gold --ignore-existing;
      /usr/bin/mc mb myminio/lakehouse-warehouse --ignore-existing;
      /usr/bin/mc mb myminio/flink-checkpoints --ignore-existing;
      /usr/bin/mc mb myminio/paimon-warehouse --ignore-existing;
      exit 0;
      "
    networks:
      - dataplatform

  # ============================================================================
  # POSTGRESQL - Metadata storage
  # ============================================================================
  postgres:
    image: postgres:16
    hostname: postgres
    container_name: postgres
    ports:
      - "5432:5432"
    environment:
      POSTGRES_DB: dataplatform_metadata
      POSTGRES_USER: dataplatform
      POSTGRES_PASSWORD: dataplatform_password
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./scripts/setup/init_postgres.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - dataplatform
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U dataplatform"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ============================================================================
  # APACHE FLINK - Stream processing
  # ============================================================================
  flink-jobmanager:
    image: flink:1.18.0-scala_2.12-java11
    hostname: flink-jobmanager
    container_name: flink-jobmanager
    ports:
      - "8082:8081"
    command: jobmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
        taskmanager.numberOfTaskSlots: 4
        parallelism.default: 2
        state.backend: rocksdb
        state.checkpoints.dir: s3://lakehouse/flink-checkpoints
        state.savepoints.dir: s3://lakehouse/flink-savepoints
        execution.checkpointing.interval: 60000
        s3.endpoint: http://minio:9000
        s3.access-key: minioadmin
        s3.secret-key: minioadmin
        s3.path.style.access: true
    volumes:
      - flink-jobmanager-data:/tmp
    networks:
      - dataplatform

  flink-taskmanager:
    image: flink:1.18.0-scala_2.12-java11
    hostname: flink-taskmanager
    container_name: flink-taskmanager
    depends_on:
      - flink-jobmanager
    command: taskmanager
    scale: 2
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
        taskmanager.numberOfTaskSlots: 4
        parallelism.default: 2
        state.backend: rocksdb
        state.checkpoints.dir: s3://lakehouse/flink-checkpoints
        state.savepoints.dir: s3://lakehouse/flink-savepoints
        s3.endpoint: http://minio:9000
        s3.access-key: minioadmin
        s3.secret-key: minioadmin
        s3.path.style.access: true
    volumes:
      - flink-taskmanager-data:/tmp
    networks:
      - dataplatform

  # ============================================================================
  # QDRANT - Vector database
  # ============================================================================
  qdrant:
    image: qdrant/qdrant:latest
    hostname: qdrant
    container_name: qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant-data:/qdrant/storage
    networks:
      - dataplatform

  # ============================================================================
  # REDIS - Caching
  # ============================================================================
  redis:
    image: redis:7-alpine
    hostname: redis
    container_name: redis
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes
    volumes:
      - redis-data:/data
    networks:
      - dataplatform

  # ============================================================================
  # PROMETHEUS - Metrics collection
  # ============================================================================
  prometheus:
    image: prom/prometheus:latest
    hostname: prometheus
    container_name: prometheus
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    volumes:
      - ./configs/dev/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    networks:
      - dataplatform

  # ============================================================================
  # GRAFANA - Monitoring dashboards
  # ============================================================================
  grafana:
    image: grafana/grafana:latest
    hostname: grafana
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_INSTALL_PLUGINS: grafana-piechart-panel
    volumes:
      - grafana-data:/var/lib/grafana
      - ./configs/dev/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./configs/dev/grafana/datasources:/etc/grafana/provisioning/datasources
    networks:
      - dataplatform
    depends_on:
      - prometheus

  # ============================================================================
  # KAFKA UI - Kafka management interface
  # ============================================================================
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    hostname: kafka-ui
    container_name: kafka-ui
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka-1:29092,kafka-2:29093,kafka-3:29094
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
      KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://schema-registry:8081
    depends_on:
      - kafka-1
      - kafka-2
      - kafka-3
      - schema-registry
    networks:
      - dataplatform

# ==============================================================================
# VOLUMES
# ==============================================================================
volumes:
  zookeeper-data:
  zookeeper-logs:
  kafka-1-data:
  kafka-2-data:
  kafka-3-data:
  minio-data:
  postgres-data:
  flink-jobmanager-data:
  flink-taskmanager-data:
  qdrant-data:
  redis-data:
  prometheus-data:
  grafana-data:

# ==============================================================================
# NETWORKS
# ==============================================================================
networks:
  dataplatform:
    driver: bridge
